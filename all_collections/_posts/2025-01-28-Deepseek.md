---
layout: post
title: Playing with deepseek
date: 2025-01-28
categories: ["tech", "learning", "deepseek", "AI"]
---


# Deepseek on your local

Trying to be in, I look for a way to play with deepseek.

What makes this model great (besides its resource efficiency) is that it's open-source. You can install it in your computer and run it withouth sharing any information. If you want to ask about Tianamen, this could be your way (as far as I understand, censorship works at the app level, not model level, even though this hasn't work for me yet).

>Moreover, the fact that it is available and open-source also means that any of us can download it and run it on our own computer. It might be a bit slower, but it ensures that everything you write and interact with stays on your device, preventing the Chinese company from accessing it. In short, everything stays on your PC. This is in stark contrast to the secrecy and lack of freedom of some private models.

[From Xataka - ESP](https://www.xataka.com/basics/deepseek-que-como-funciona-que-opciones-tiene-esta-inteligencia-artificial)

After looking around a bit, I came across this docker-compose file in the Open WebUI repository that allows you to run the model without any extra installations, but it mounts a volume so that the data persists (this way, you'll see your history, but only you, as it will be stored locally). In the volumes parameter of both services, you can change the default ollama and open-webui to the path of your choice, and that's where the model will be saved.

Open web-ui provides a graphical interface for the models your are running, and also you can store your own configurations.

```
services:
  ollama:
    volumes:
      - ollama:/root/.ollama
    container_name: ollama
    pull_policy: always
    tty: true
    restart: unless-stopped
    image: ollama/ollama:${OLLAMA_DOCKER_TAG-latest}

  open-webui:
    image: ghcr.io/open-webui/open-webui:${WEBUI_DOCKER_TAG-main}
    container_name: open-webui
    volumes:
      - open-webui:/app/backend/data
    ports:
      - ${OPEN_WEBUI_PORT-3000}:8080
    depends_on:
      - ollama
    environment:
      - 'OLLAMA_BASE_URL=http://ollama:11434'
      - 'WEBUI_SECRET_KEY='
      - 'DEFAULT_MODELS=deepseek-r1'
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped

volumes:
  ollama: {}
  open-webui: {}
```

To make this live, if you already have docker installed, just run.
```
docker-compose up -d 
```

To download the model the first time, I entered the Ollama container and ran it once from there.
```
docker exec -it ollama /bin/bash
ollama run deepseek-r1
```

You can find other models to run with ollama and open web-ui [here](https://ollama.com/library/deepseek-r1)


There are [many params to explore](https://docs.openwebui.com/getting-started/advanced-topics/env-configuration), like security (set or enable offline mode), connection to other apis, prompt completition, web search, google drive integration, etc.

It's really slow, but it works!
Next step will be [working on performance.](https://docs.docker.com/compose/how-tos/gpu-support/)

If you want to customize the docker-compose, just remember to put it down and then up again to recreate and have your changes
```
docker-compose down
```

### Running directly from your computer

If you don't want to run in in a docker container, you can just install ollama and pull it from there:
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama run deepseek-r1
```

And you will have it running on your terminal. 


### References:
<https://platzi.com/blog/deepseek-r1-instalar-local/>  
<https://github.com/ollama/ollama/tree/main>  
<https://github.com/open-webui/open-webui/tree/main>  
<https://medium.com/@edu.ukulelekim/how-to-locally-deploy-ollama-and-open-webui-with-docker-compose-318f0582e01f>  
