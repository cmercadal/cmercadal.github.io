---
layout: post
title: Playing with deepseek
date: 2025-01-28
categories: ["tech", "learning", "deepseek", "UI"]
---


# Deepseek on your local

Trying to be in, I look for a way to play with deepseek.

What makes this model great (besides its resource efficiency) is that it's open-source. You can install it in your computer and run it withouth sharing any information. If you want to ask about Tianamen, this could be your way (as far as I understand, censorship works at the app level, not model level, even though this hasn't work for me yet).

>Además, el hecho de que esté disponible y su código sea abierto también significa que cualquiera de nosotros puedes descargarlo y hacerlo >funcionar en su propio ordenador. Quizá esto haga que vaya más lento, pero permite que todo lo que escribas e interactúes se quede en tu equipo, >y que la empresa china no acceda a ello. Vamos, todo se queda en tu PC. Esto contrasta totalmente con el hermetismo y la poca libertad de unos >modelos privados.
> [From Xataka](https://www.xataka.com/basics/deepseek-que-como-funciona-que-opciones-tiene-esta-inteligencia-artificial)

After looking around a bit, I came across this docker-compose file in the Open WebUI repository that allows you to run the model without any extra installations, but it mounts a volume so that the data persists (this way, you'll see your history, but only you, as it will be stored locally). In the volumes parameter of both services, you can change the default ollama and open-webui to the path of your choice, and that's where the model will be saved.

Open web-ui provides a graphical interface for the models your are running, and also you can store your own configurations.

```
services:
  ollama:
    volumes:
      - ollama:/root/.ollama
    container_name: ollama
    pull_policy: always
    tty: true
    restart: unless-stopped
    image: ollama/ollama:${OLLAMA_DOCKER_TAG-latest}

  open-webui:
    image: ghcr.io/open-webui/open-webui:${WEBUI_DOCKER_TAG-main}
    container_name: open-webui
    volumes:
      - open-webui:/app/backend/data
    ports:
      - ${OPEN_WEBUI_PORT-3000}:8080
    depends_on:
      - ollama
    environment:
      - 'OLLAMA_BASE_URL=http://ollama:11434'
      - 'WEBUI_SECRET_KEY='
      - 'DEFAULT_MODELS=deepseek-r1'
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped

volumes:
  ollama: {}
  open-webui: {}
```

To make this live, if you already have docker installed, just run.
```
docker-compose up -d 
```

To download the model the first time, I entered the Ollama container and ran it once from there.
```
docker exec -it ollama /bin/bash #para entrar a la terminal del container
```
```
ollama run deepseek-r1
```

You can find other models to run with ollama and open web-ui [here](https://ollama.com/library/deepseek-r1)


There are many params to explore, like security (set or enable offline mode), connection to other apis, prompt completition, web search, google drive integration, etc: https://docs.openwebui.com/getting-started/advanced-topics/env-configuration

It's really slow, but it works!
Next step will be working on performance. Starting point: https://docs.docker.com/compose/how-tos/gpu-support/

If you want to customize the docker-compose, just remember to put it down and then up again to recreate and have your changes
```
docker-compose down
```

References:
https://platzi.com/blog/deepseek-r1-instalar-local/
https://github.com/ollama/ollama/tree/main
https://github.com/open-webui/open-webui/tree/main
https://medium.com/@edu.ukulelekim/how-to-locally-deploy-ollama-and-open-webui-with-docker-compose-318f0582e01f
